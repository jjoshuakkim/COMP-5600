{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMo3Domssy6zFqY7sWAuUCZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjoshuakkim/COMP-5600/blob/main/assignment_5_Joshua_Kim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Softmax Function Derivative**\n",
        "\n",
        "Attached to pdf"
      ],
      "metadata": {
        "id": "8iiyaBp4ALwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Function Derivative**\n",
        "\n",
        "Attached to pdf\n",
        "\n",
        "Reference: https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/"
      ],
      "metadata": {
        "id": "V1RWD7MEAgOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MDP**\n",
        "\n",
        "State Space: The state space can be defined as the space where all postions can be accessed by the agent (25x25 grid meaning the state space has 625 states). Each state can be represented as a tuple (x, y) as coordinates in the grid\n",
        "\n",
        "Action Space: We can represent the action space as A = {up, down, left, right}\n",
        "\n",
        "Transition Probabilities: Given a state and action, the transition to the next state depends on the action taken. If the action leads to a blocked cell or goes out of bounds, the agent remains in the same state. Otherwise, the agent moves according to the chosen action. We can represent the transition probabilities as P(s'|s, a), where s in the current state, a is the action taken, and s' is the next state.\n",
        "\n",
        "Reward Function: The agent receives a reward of -1 for each time step until it reaches the goal state. The reward function can be denoted as R(s, a, s'), where s is the current state, a is the action taken. and s' is the next state.\n",
        "\n",
        "Discount Factor: y (value between 0 and 1) is used to weigh the importance of future rewards.\n",
        "\n",
        "Reference: I did use chatgpt to help me debug my policy iteration algorithm and also this reference https://www.youtube.com/watch?v=RlugupBiC6w"
      ],
      "metadata": {
        "id": "jDp-N-P5Ahye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation of Policy Iteration**"
      ],
      "metadata": {
        "id": "J-2zNh201v0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# State space\n",
        "states = [(x, y) for x in range(1, 26) for y in range(1, 26)]\n",
        "\n",
        "# Action space\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "\n",
        "# Transition probabilities\n",
        "def transition_probabilities(state, action):\n",
        "    # Implement transition probabilities\n",
        "    x, y = state\n",
        "    if action == 'up':\n",
        "        return {(x, min(y+1, 25)): 0.7, (x, max(y-1, 1)): 0.1, (max(x-1, 1), y): 0.1, (min(x+1, 25), y): 0.1}\n",
        "    elif action == 'down':\n",
        "        return {(x, max(y-1, 1)): 0.7, (x, min(y+1, 25)): 0.1, (max(x-1, 1), y): 0.1, (min(x+1, 25), y): 0.1}\n",
        "    elif action == 'left':\n",
        "        return {(max(x-1, 1), y): 0.7, (x, min(y+1, 25)): 0.1, (x, max(y-1, 1)): 0.1, (min(x+1, 25), y): 0.1}\n",
        "    elif action == 'right':\n",
        "        return {(min(x+1, 25), y): 0.7, (x, min(y+1, 25)): 0.1, (x, max(y-1, 1)): 0.1, (max(x-1, 1), y): 0.1}\n",
        "\n",
        "# Reward function\n",
        "def reward(state, action, next_state):\n",
        "    x, y = state\n",
        "    if (x, y) == (1, 1):  # Goal state\n",
        "        return 100\n",
        "    elif (x, y) == (24, 24):  # Obstacle state\n",
        "        return -100\n",
        "    else:\n",
        "        return -1  # Step cost\n",
        "\n",
        "# Discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "# Initialize policy\n",
        "policy = {state: np.random.choice(actions) for state in states}\n",
        "\n",
        "# Policy iteration algorithm\n",
        "def policy_iteration():\n",
        "    policy_stable = False\n",
        "    while not policy_stable:\n",
        "        # Policy evaluation\n",
        "        V = {state: 0 for state in states}\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for state in states:\n",
        "                action = policy[state]\n",
        "                value = 0\n",
        "                for next_state, prob in transition_probabilities(state, action).items():\n",
        "                    r = reward(state, action, next_state)\n",
        "                    value += prob * (r + gamma * V[next_state])\n",
        "                delta = max(delta, abs(V[state] - value))\n",
        "                V[state] = value\n",
        "            if delta < 1e-6:\n",
        "                break\n",
        "\n",
        "        # Policy improvement\n",
        "        policy_stable = True\n",
        "        for state in states:\n",
        "            old_action = policy[state]\n",
        "            max_value = float('-inf')\n",
        "            for action in actions:\n",
        "                value = 0\n",
        "                for next_state, prob in transition_probabilities(state, action).items():\n",
        "                    r = reward(state, action, next_state)\n",
        "                    value += prob * (r + gamma * V[next_state])\n",
        "                if value > max_value:\n",
        "                    max_value = value\n",
        "                    best_action = action\n",
        "            if best_action != old_action:\n",
        "                policy_stable = False\n",
        "            policy[state] = best_action\n",
        "\n",
        "        if policy_stable:\n",
        "            break\n",
        "\n",
        "    return V, policy\n",
        "\n",
        "# Run policy iteration\n",
        "optimal_value_function, optimal_policy = policy_iteration()\n",
        "\n",
        "# Visualize\n",
        "print(\"Optimal Policy:\")\n",
        "for y in range(1, 26):\n",
        "    for x in range(1, 26):\n",
        "        state = (x, y)\n",
        "        action = optimal_policy[state]\n",
        "        if state == (1, 1):\n",
        "            print(\"G\", end=\"\\t\")  # Goal state\n",
        "        elif state == (12, 12):\n",
        "            print(\"X\", end=\"\\t\")  # Obstacle state\n",
        "        elif action == 'up':\n",
        "            print(\"↑\", end=\"\\t\")\n",
        "        elif action == 'down':\n",
        "            print(\"↓\", end=\"\\t\")\n",
        "        elif action == 'left':\n",
        "            print(\"←\", end=\"\\t\")\n",
        "        elif action == 'right':\n",
        "            print(\"→\", end=\"\\t\")\n",
        "    print()\n",
        "\n",
        "# Display the optimal value function\n",
        "print(\"\\nOptimal Value Function:\")\n",
        "for y in range(1, 26):\n",
        "    for x in range(1, 26):\n",
        "        state = (x, y)\n",
        "        value = optimal_value_function[state]\n",
        "        print(f\"{value:.2f}\", end=\"\\t\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "k2yUXzuovM-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64fd448f-465b-43be-fbf5-de3b0d3ee9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            "G\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t→\t→\t↓\t\n",
            "↓\t↓\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t↓\t↓\t\n",
            "↓\t↓\t↓\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t↓\t↓\t\n",
            "↓\t↓\t↓\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t↓\t↓\t\n",
            "↓\t↓\t↓\t↓\t↓\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t↓\t↓\t\n",
            "↓\t↓\t↓\t↓\t↓\t↓\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t↓\t↓\t↓\t\n",
            "↓\t↓\t↓\t↓\t↓\t↓\t↓\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t↓\t↓\t↓\t\n",
            "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t↓\t↓\t↓\t↓\t\n",
            "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t↓\t↓\t↓\t↓\t\n",
            "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t↓\t↓\t↓\t↓\t↓\t\n",
            "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t←\t←\t←\t←\t←\t←\t←\t←\t↓\t↓\t↓\t↓\t↓\t↓\t\n",
            "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\tX\t←\t←\t←\t←\t←\t←\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t\n",
            "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t←\t←\t←\t←\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t\n",
            "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t←\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t\n",
            "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t←\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t\n",
            "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t←\t←\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t\n",
            "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t←\t←\t←\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t\n",
            "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t←\t←\t←\t←\t←\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t\n",
            "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t←\t←\t←\t←\t←\t←\t←\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t\n",
            "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t↓\t↓\t↓\t↓\t↓\t\n",
            "↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t↓\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t↓\t↓\t↓\t↓\t↓\t\n",
            "↓\t↓\t↓\t↓\t↓\t↓\t↓\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t↓\t↓\t↓\t\n",
            "↑\t↓\t↓\t↓\t↓\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t↓\t↓\t↓\t\n",
            "↑\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t↑\t↑\t\n",
            "↑\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t←\t→\t↑\t\n",
            "\n",
            "Optimal Value Function:\n",
            "284.46\t234.53\t193.48\t159.60\t131.57\t108.30\t88.96\t72.85\t59.42\t48.20\t38.82\t30.97\t24.40\t18.89\t14.28\t10.40\t7.15\t4.43\t2.13\t0.21\t-1.40\t-2.71\t-3.36\t-2.32\t-0.79\t\n",
            "234.53\t197.76\t166.08\t138.96\t115.86\t96.25\t79.63\t65.58\t53.71\t43.70\t35.25\t28.13\t22.13\t17.07\t12.81\t9.22\t6.19\t3.65\t1.50\t-0.31\t-1.83\t-3.08\t-3.96\t-3.42\t-2.34\t\n",
            "193.48\t166.08\t139.66\t117.13\t97.93\t81.57\t67.64\t55.80\t45.74\t37.20\t29.95\t23.80\t18.59\t14.18\t10.45\t7.28\t4.61\t2.35\t0.43\t-1.18\t-2.55\t-3.69\t-4.54\t-4.35\t-3.61\t\n",
            "159.60\t138.96\t117.13\t98.07\t81.85\t68.04\t56.30\t46.31\t37.81\t30.58\t24.44\t19.22\t14.78\t11.01\t7.81\t5.10\t2.79\t0.84\t-0.82\t-2.22\t-3.41\t-4.41\t-5.17\t-5.15\t-4.65\t\n",
            "131.57\t115.86\t97.93\t81.85\t68.07\t56.36\t46.41\t37.94\t30.74\t24.62\t19.41\t14.98\t11.22\t8.02\t5.30\t2.99\t1.02\t-0.65\t-2.06\t-3.27\t-4.29\t-5.14\t-5.81\t-5.84\t-5.51\t\n",
            "108.30\t96.25\t81.57\t68.04\t56.36\t46.41\t37.96\t30.76\t24.65\t19.45\t15.03\t11.27\t8.08\t5.36\t3.05\t1.09\t-0.58\t-2.00\t-3.20\t-4.23\t-5.10\t-5.83\t-6.38\t-6.44\t-6.23\t\n",
            "88.96\t79.63\t67.64\t56.30\t46.41\t37.96\t30.77\t24.65\t19.46\t15.04\t11.29\t8.09\t5.38\t3.07\t1.11\t-0.56\t-1.98\t-3.18\t-4.20\t-5.08\t-5.81\t-6.43\t-6.89\t-6.96\t-6.82\t\n",
            "72.85\t65.58\t55.80\t46.31\t37.94\t30.76\t24.65\t19.46\t15.04\t11.29\t8.10\t5.38\t3.08\t1.11\t-0.55\t-1.97\t-3.17\t-4.20\t-5.07\t-5.81\t-6.44\t-6.96\t-7.32\t-7.40\t-7.32\t\n",
            "59.42\t53.71\t45.74\t37.81\t30.74\t24.65\t19.46\t15.04\t11.29\t8.10\t5.38\t3.08\t1.12\t-0.55\t-1.97\t-3.17\t-4.20\t-5.07\t-5.81\t-6.43\t-6.97\t-7.40\t-7.70\t-7.78\t-7.74\t\n",
            "48.20\t43.70\t37.20\t30.58\t24.62\t19.45\t15.04\t11.29\t8.10\t5.38\t3.08\t1.12\t-0.55\t-1.97\t-3.17\t-4.20\t-5.07\t-5.81\t-6.43\t-6.97\t-7.42\t-7.79\t-8.03\t-8.11\t-8.09\t\n",
            "38.82\t35.25\t29.95\t24.44\t19.41\t15.03\t11.29\t8.10\t5.38\t3.08\t1.12\t-0.55\t-1.97\t-3.17\t-4.20\t-5.07\t-5.81\t-6.43\t-6.97\t-7.42\t-7.80\t-8.11\t-8.32\t-8.39\t-8.38\t\n",
            "30.97\t28.13\t23.80\t19.22\t14.98\t11.27\t8.09\t5.38\t3.08\t1.12\t-0.55\t-1.97\t-3.17\t-4.20\t-5.07\t-5.81\t-6.43\t-6.97\t-7.42\t-7.81\t-8.13\t-8.39\t-8.56\t-8.63\t-8.63\t\n",
            "24.40\t22.13\t18.59\t14.78\t11.22\t8.08\t5.38\t3.08\t1.12\t-0.55\t-1.97\t-3.17\t-4.20\t-5.07\t-5.81\t-6.43\t-6.97\t-7.42\t-7.81\t-8.14\t-8.41\t-8.62\t-8.77\t-8.83\t-8.84\t\n",
            "18.89\t17.07\t14.18\t11.01\t8.02\t5.36\t3.07\t1.11\t-0.55\t-1.97\t-3.17\t-4.20\t-5.07\t-5.81\t-6.43\t-6.97\t-7.42\t-7.81\t-8.14\t-8.42\t-8.65\t-8.83\t-8.95\t-9.00\t-9.01\t\n",
            "14.28\t12.81\t10.45\t7.81\t5.30\t3.05\t1.11\t-0.55\t-1.97\t-3.17\t-4.20\t-5.07\t-5.81\t-6.43\t-6.97\t-7.42\t-7.81\t-8.14\t-8.42\t-8.65\t-8.85\t-9.00\t-9.10\t-9.15\t-9.16\t\n",
            "10.40\t9.22\t7.28\t5.10\t2.99\t1.09\t-0.56\t-1.97\t-3.17\t-4.20\t-5.07\t-5.81\t-6.43\t-6.97\t-7.42\t-7.81\t-8.14\t-8.42\t-8.65\t-8.85\t-9.02\t-9.15\t-9.23\t-9.28\t-9.29\t\n",
            "7.15\t6.19\t4.61\t2.79\t1.02\t-0.58\t-1.98\t-3.17\t-4.20\t-5.07\t-5.81\t-6.43\t-6.97\t-7.42\t-7.81\t-8.14\t-8.42\t-8.65\t-8.86\t-9.02\t-9.16\t-9.27\t-9.35\t-9.38\t-9.40\t\n",
            "4.43\t3.65\t2.35\t0.84\t-0.65\t-2.00\t-3.18\t-4.20\t-5.07\t-5.81\t-6.43\t-6.97\t-7.42\t-7.81\t-8.14\t-8.42\t-8.65\t-8.86\t-9.03\t-9.17\t-9.29\t-9.38\t-9.44\t-9.47\t-9.49\t\n",
            "2.13\t1.50\t0.43\t-0.82\t-2.06\t-3.20\t-4.20\t-5.07\t-5.81\t-6.43\t-6.97\t-7.42\t-7.81\t-8.14\t-8.42\t-8.65\t-8.86\t-9.03\t-9.17\t-9.29\t-9.39\t-9.47\t-9.52\t-9.55\t-9.56\t\n",
            "0.21\t-0.31\t-1.18\t-2.22\t-3.27\t-4.23\t-5.08\t-5.81\t-6.43\t-6.97\t-7.42\t-7.81\t-8.14\t-8.42\t-8.65\t-8.85\t-9.02\t-9.17\t-9.29\t-9.40\t-9.48\t-9.55\t-9.60\t-9.63\t-9.64\t\n",
            "-1.40\t-1.83\t-2.55\t-3.41\t-4.29\t-5.10\t-5.81\t-6.44\t-6.97\t-7.42\t-7.80\t-8.13\t-8.41\t-8.65\t-8.85\t-9.02\t-9.16\t-9.29\t-9.39\t-9.48\t-9.56\t-9.63\t-9.70\t-9.79\t-9.73\t\n",
            "-2.71\t-3.08\t-3.69\t-4.41\t-5.14\t-5.83\t-6.43\t-6.96\t-7.40\t-7.79\t-8.11\t-8.39\t-8.62\t-8.83\t-9.00\t-9.15\t-9.27\t-9.38\t-9.47\t-9.55\t-9.63\t-9.74\t-10.03\t-10.80\t-10.06\t\n",
            "-3.36\t-3.96\t-4.54\t-5.17\t-5.81\t-6.38\t-6.89\t-7.32\t-7.70\t-8.03\t-8.32\t-8.56\t-8.77\t-8.95\t-9.10\t-9.23\t-9.35\t-9.44\t-9.52\t-9.60\t-9.70\t-10.03\t-11.87\t-20.29\t-11.66\t\n",
            "-2.32\t-3.42\t-4.35\t-5.15\t-5.84\t-6.44\t-6.96\t-7.40\t-7.78\t-8.11\t-8.39\t-8.63\t-8.83\t-9.00\t-9.15\t-9.28\t-9.38\t-9.47\t-9.55\t-9.63\t-9.79\t-10.80\t-20.29\t-115.23\t-16.08\t\n",
            "-0.79\t-2.34\t-3.61\t-4.65\t-5.51\t-6.23\t-6.82\t-7.32\t-7.74\t-8.09\t-8.38\t-8.63\t-8.84\t-9.01\t-9.16\t-9.29\t-9.40\t-9.49\t-9.56\t-9.64\t-9.73\t-10.06\t-11.66\t-16.08\t-3.51\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Report**\n",
        "\n",
        "When implementing the MDP, I chose a number of data structures to represent different parts of the problem. I represented the state space as a list of tuples, where each tuple represents a distinct state in the grid world. The action space was represented as a simple list of strings. The transition probabilities are represented as a dictionary to show the transition from one state to another given an action. The policy was also represented as a dictionary, where the keys are states and its items are the chosen actions for each state. The value function was represented as a dictionary. The keys are states, and the values are the estimated values of those states. The chosen data structures were super flexible and adaptable to different problem scenarios, so it was really easy to modify and extend as the problem requirements grew.\n",
        "\n",
        "The problem being addressed was to find the optimal policy for an agent to take in in order to maximize the expected cumulative reward in an MDP environment. In my implementation, the transition probabilities are stochastic (meaning a 70% chance of moving in the intended direction and a 10% chance of moving in each of the other directions. The reward function assigns a reward of 100 for reaching the goal state and a penalty of -100 for entering the obstacle state, and a step cost of -1 for all other transitions. I also used a discount  factor to ensure the that agent prefers to reach the goal state as quickly as it can.\n",
        "\n",
        "In my policy evaluation, it's supposed to compute the value function for the current policy by solving a system of linear equations. The value function represents how good it is for the agent to be in a particular state when following the current policy. In the policy improvement, the algorithm updates the policy by choosing the action that maximizes the expected future reward for each state, based on the value function computed in the previous step.\n",
        "\n",
        "The algorithm iterates between these two steps until the policy converges (no further improvements) and the optimal policy has been found."
      ],
      "metadata": {
        "id": "ewEYldxN11tU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random policy\n",
        "def random_policy(state):\n",
        "    return np.random.choice(actions)\n",
        "\n",
        "def evaluate_random_policy(start_states):\n",
        "    steps_to_goal = []\n",
        "    total_states = 25 * 25\n",
        "    for start_state in start_states:\n",
        "        state = start_state\n",
        "        steps = 0\n",
        "        while state != (1, 1):\n",
        "            action = random_policy(state)\n",
        "            next_state, prob = next(iter(transition_probabilities(state, action).items()))\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "            if steps > total_states:\n",
        "                print(f\"Loop encountered for start state {start_state}. Terminating execution.\")\n",
        "                break\n",
        "        else:\n",
        "            steps_to_goal.append(steps)\n",
        "    return np.median(steps_to_goal)\n",
        "\n",
        "# Choose 3 random start states\n",
        "start_states = [(np.random.randint(1, 26), np.random.randint(1, 26)) for _ in range(3)]\n",
        "\n",
        "# Print the randomly chosen start states\n",
        "print(\"Randomly chosen start states:\")\n",
        "for state in start_states:\n",
        "    print(state)\n",
        "\n",
        "# Evaluate random policy\n",
        "random_policy_performance = evaluate_random_policy(start_states)\n",
        "print(f\"\\nMedian number of steps to reach the goal state with a random policy: {random_policy_performance}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxEIRdkEjE91",
        "outputId": "84da641d-21e7-4f32-a9ac-87080d94b6c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Randomly chosen start states:\n",
            "(22, 15)\n",
            "(1, 19)\n",
            "(5, 9)\n",
            "Loop encountered for start state (22, 15). Terminating execution.\n",
            "Loop encountered for start state (1, 19). Terminating execution.\n",
            "Loop encountered for start state (5, 9). Terminating execution.\n",
            "\n",
            "Median number of steps to reach the goal state with a random policy: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy(policy, start_states):\n",
        "    steps_to_goal = []\n",
        "    for start_state in start_states:\n",
        "        state = start_state\n",
        "        steps = 0\n",
        "        while state != (1, 1):\n",
        "            action = policy[state]\n",
        "            next_state, prob = next(iter(transition_probabilities(state, action).items()))\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "        steps_to_goal.append(steps)\n",
        "    return np.median(steps_to_goal)\n",
        "\n",
        "# Evaluate the intermediate policy\n",
        "intermediate_policy_performance = evaluate_policy(policy, start_states)\n",
        "\n",
        "# Print the randomly chosen start states\n",
        "print(\"Randomly chosen start states:\")\n",
        "for state in start_states:\n",
        "    print(state)\n",
        "\n",
        "# Print the result\n",
        "print(f\"\\nMedian number of steps to reach the goal state with the intermediate policy: {intermediate_policy_performance}\")\n"
      ],
      "metadata": {
        "id": "9XHg3bxcnQYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84490893-e161-4fb8-8885-a654cdcc2501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Randomly chosen start states:\n",
            "(12, 1)\n",
            "(20, 8)\n",
            "(22, 13)\n",
            "\n",
            "Median number of steps to reach the goal state with the intermediate policy: 26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate optimal policy\n",
        "optimal_policy_performance = evaluate_policy(optimal_policy, start_states)\n",
        "\n",
        "# Print the result\n",
        "print(f\"\\nMedian number of steps to reach the goal state with the optimal policy: {optimal_policy_performance}\")\n"
      ],
      "metadata": {
        "id": "b6TmEah-npKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c321265e-5f81-4290-fc33-5805dcc001a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Median number of steps to reach the goal state with the optimal policy: 26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results**\n",
        "\n",
        "As you can see in the results, the random policy steps is may hit a loop in the traversal, and the intermediate and optimal policy steps are equal, which is an extremely rare case. I realized it was almost impossible to evaluate the performance of a random policy without setting a clear start state near the goal. Initially, I had the algorithm keep running until it did end up finding the goal state; however, the median always ended up being abnormally high. I ended up having to set a max step count where if you step more than the total number of states, then you know you've hit a loop, then you cut the execution."
      ],
      "metadata": {
        "id": "GHh8l_ERvym9"
      }
    }
  ]
}